version: "1.0"

# Mandatory: Replication group name - identifier for the replication job
replication_group: "readme_example_group"

# Mandatory: Audit logging configuration
audit_config:
  # Mandatory: audit table full name
  audit_table: "data_replication.audit.logging"
  # Mandatory: if logged at source or target. default to target
  logging_workspace: "target"
  # Optional: whether to create audit catalog if it does not exist. default is false.
  create_audit_catalog: false
  # Optional: audit catalog location. default is None.
  audit_catalog_location: null

# Mandatory if storage credential are replicated
storage_credential_config:
  # Mandatory if storage credential are replicated. Accepted values: aws, azure_access_connector, azure_managed_identity
  target_credential_type: "aws"
  # Mandatory if storage credential are replicated. mapping between source storage credential name and cloud principal id on target.
  # for aws, cloud principal id is iam role arn. for azure, it is azure managed identity.
  mapping:
    "source_storage_credential_1": "iam_role_arn_1"
    "source_storage_credential_2": "iam_role_arn_2"

# Mandatory if exterrnal location, catalog, schema, tables or volumes with locations are replicated. cloud storage mapping between source and target cloud
cloud_url_mapping:
  "abfss://test@oneenvadls.dfs.core.windows.net/": "s3://one-env-uc-external-location/test/"

# Optional: External location configuration
external_location_config:
  # Optional: external location to replicate. if not provided, all external locations matched in cloud_url_mapping will be replicated.
  external_locations:
    - external_location_1
    - external_location_2

# Mandatory: Only one of uc_object_types, table_types, volume_types must be provided
# when uc_object_types is provided, uc metadata will be replicated.
# when table_types is provided, data in tables of specified types will be replicated.
# when volume_types is provided, files in volumes of specified types will be replicated.

# Optional: replication group level uc object types. 
uc_object_types: [
    "all",
    "storage_credential",
    "external_location",
    "catalog",
    "catalog_tag",
    "schema",
    "schema_tag",
    "view",
    "volume",
    "volume_tag",
    "table_tag",
    "column_tag",
    "column_comment",
    # "table_comment"
    # "permission"
  ]

# Optional: replication group level table types. 
# supported types: all, streaming_table, managed, external.
table_types: ["all", "managed", "streaming_table", "external"]

# Optional: replication group level volume types. 
# supported volume types: managed, external.
# volume_types: ["all","managed", "external"]

# Mandatory: At least one of backup_config, replication_config, reconciliation_config must be provided at catalog level or replication group level (lower priority).

# Optional: backup configuration (at replication group level)
backup_config:
  # Mandatory at replication group level or catalog level: whether to backup catalog.
  enabled: true
  # Optional: source catalog for backup. if not provided, same as target catalog.
  source_catalog: "catalog_example"
  # Optional: create delta share recipient. default is false.
  create_recipient: false
  # Optional: recipient name. default to <target_databricks_connect_config.name>_recipient
  recipient_name: "aws_recipient"
  # Optional: whether to create share on source.default is false.
  create_share: false
  # Optional: whether to add the schema to share. default is false.
  add_to_share: false
  # Optional: share name for source catalog. default to <catalog_name>_to_<target_databricks_connect_config.name>_share
  share_name: "catalog_example_to_aws_share"

  # Streaming table specific configurations start here: ------------------------
  # Optional: whether to create backup catalog on target. default is false.
  create_backup_catalog: false
  # Optional: backup catalog name. must not be set for non-streaming table replication.
  # default to __replication_internal_<catalog_name>_to_<target_databricks_connect_config.name>
  backup_catalog: "__replication_internal_catalog_example_to_aws"
  # Optional: backup catalog location. default is None.
  backup_catalog_location: null
  # Optional: prefix for backup schema names. default is None.
  backup_schema_prefix: null
  # Optional: share name for streaming table backing table. default to <backup_catalog>_share
  backup_share_name: "__replication_internal_catalog_example_to_aws_share"
  # streaming table specific configurations end here: ------------------------

# Optional: replication configuration (at replication group level)
replication_config:
  # Mandatory at replication group level or catalog level: whether to replicate catalog.
  enabled: true
  # Optional: whether to replicate the catalog if it does not exist on target.default is false.
  create_target_catalog: false
  # Optional: target catalog location. default is None.
  target_catalog_location: null
  # Optional: whether to create the shared catalog. default is false.
  create_shared_catalog: false
  # Optional: share name of source catalog. default to <catalog_name>_to_<target_databricks_connect_config.name>_share
  share_name: "catalog_example_to_aws_share"
  # Optional: catalog created from source share. default to <catalog_name>_from_<source_databricks_connect_config.name>_shared
  source_catalog: "catalog_example_from_azure_shared"
  # Optional: whether to create intermediate catalog for replication. default is false.
  create_intermediate_catalog: false
  # Optional: intermediate catalog for replication. default is None.
  intermediate_catalog: null
  # Optional: intermediate catalog location. default is None.
  intermediate_catalog_location: null
  # Optional: whether to enforce schema during replication. default is true.
  enforce_schema: true

  # uc metadata specific configurations start here: ------------------------
  # Optional: whether to replicate predictive optimization setting during UC catalog, schema and table replication.
  # default is false to disable PO at target to enable consistent incremental deep clone. Setting to true will replicate PO settings from source to target.
  replicate_enable_predictive_optimization: false
  # Optional: whether to overwrite tags during replication. default is true.
  overwrite_tags: true
  # Optional: whether to overwrite comments during replication. default is true.
  overwrite_comments: true
  # uc metadata specific configurations end here: ------------------------

  # Streaming table specific configurations start here: ------------------------
  # Optional: share name for streaming table backing table share. default to __replication_internal_<catalog_name>_to_<target_databricks_connect_config.name>_share
  backup_share_name: "__replication_internal_catalog_example_to_aws_share"
  # Optional: catalog created from backing table share. default to __replication_internal_<catalog_name>_from_<source_databricks_connect_config.name>_shared
  backup_catalog: "__replication_internal_catalog_example_from_azure_shared"
  # Streaming table specific configurations end here: ------------------------

  # External table specific configurations start here: ------------------------
  # Optional: whether to replicate external tables as managed tables on target. default is false.
  replicate_as_managed: false
  # Optional: whether to copy data during external table replication. default is true.
  copy_files: true
  # External table specific configurations end here: ------------------------

  # Volume file specific configurations start here: ------------------------
  # Optional:  configuration for volume files replication
  volume_config:
    # Optional: number of concurrent file copy tasks. default is 10.
    max_concurrent_copies: 10
    # Optional: delete data and checkpoint on target before replication. default is false.
    delete_and_reload: false
    # Optional: whether to delete checkpoint before replication. default is false.
    delete_checkpoint: false
    # Optional: folder path under volume to replicate. default is null to replicate the whole volume.
    folder_path: null
    # Optional: autoloader options dictionary for streaming table replication
    autoloader_options: null
    # Optional: streaming timeout in seconds for streaming table replication. default is 43200 seconds (12 hours).
    streaming_timeout_seconds: 43200
    # Optional: whether to create detailed file ingestion logging catalog if it does not exist. default is false.
    create_file_ingestion_logging_catalog: false
    # Optional: detailed file ingestion logging catalog name. default to same as audit catalog.
    file_ingestion_logging_catalog: "replication"
    # Optional: detailed file ingestion logging catalog location. default is None.
    file_ingestion_logging_catalog_location: null
    # Optional: detailed file ingestion logging outputs schema name. default to same as audit schema.
    file_ingestion_logging_schema: "audit"
    # Optional: detailed file ingestion logging outputs schema name. default to detail_file_ingestion_logging.
    file_ingestion_logging_table: "detail_file_ingestion_logging"
  # Volume file specific configurations end here: ------------------------

# Optional: reconciliation configuration (at replication group level)
reconciliation_config:
  # Mandatory at replication group level or catalog level: whether to perform reconciliation.
  enabled: true
  # Optional: whether to create reconciliation catalog if it does not exist. default is false.
  create_recon_catalog: false
  # Optional: reconciliation outputs catalog name. default to same as audit catalog.
  recon_outputs_catalog: "replication"
  # Optional: reconciliation catalog location. default is None.
  recon_catalog_location: null
  # Optional: reconciliation outputs schema name. default to same as audit schema.
  recon_outputs_schema: "audit"
  # Optional: reconciliation outputs table name for schema check mismatches. default is recon_schema_comparison.
  recon_schema_check_table: "recon_schema_comparison"
  # Optional: reconciliation outputs table name for missing data mismatches. default is recon_missing_data_comparison.
  recon_missing_data_table: "recon_missing_data_comparison"
  # Optional: whether to create the shared catalog. default is false.
  create_shared_catalog: false
  # Optional: share name. default to <catalog_name>_to_<target_databricks_connect_config.name>_share
  share_name: "catalog_example_to_aws_share"
  # Optional: shared catalog name/source catalog name. default to <catalog_name>_from_<source_databricks_connect_config.name>_shared
  source_catalog: "catalog_example_from_azure_shared"
  # Optional: columns to exclude from reconciliation. default is empty list.
  exclude_columns: []
  # Optional: filter expressions for source tables. default is none.
  source_filter_expression: null
  # Optional: filter expressions for target tables. default is none.
  target_filter_expression: null
  # Optional: enable schema checks
  schema_check: true
  # Optional: enable row count checks if schema_check passes
  row_count_check: true
  # Optional: enable missing data checks if row count check passes
  missing_data_check: true
  # Optional: threshold percentage for reconciliation pass. default is 100%.
  threshold: 100
  # Optional: enable sampling for missing data check to improve performance. default is false.
  enable_sampling: false
  # Optional: number of sampling tables if sampling is enabled. default is 10.
  no_of_sampling_tables: 10

# Optional: target catalog configurations list. can be overriden by args
target_catalogs:
  # Mandatory: target catalog name
  - catalog_name: "catalog_example"
    # Optional catalog level: supported table types: all, streaming_table, managed, external.
    table_types: ["all", "managed", "streaming_table", "external"]
    # Optional catalog level: supported volume types: managed, external.
    # Note: external volumes require cloud_url_mapping configuration above
    # supported volume types: managed, external.
    volume_types: ["all", "managed", "external"]
    # Optional: WHERE clause condition used to filter schemas and tables in all schemas from {catalog}.information_schema.tables. config target_schemas, exclude_schemas at the same time not allowed.
    schema_table_filter_expression: "table_name like 'fact_%' and table_schema like 'bronze_%'"
    # Optional: target_schema list. can be overriden by args
    target_schemas:
      # Optional: target schema name list. if not provided, all schemas are included based on schema_table_filter_expression or all schemas in catalog.
      - schema_name: "bonze_1"
      - schema_name: "bonze_2"
        # Optional: include specific tables in the schema. if not provided, all tables are included. can be overriden by args
        tables:
          - table_name: "product_metrics"
          - table_name: "sales_data"
        # Optional: include specific volumes in the schema. if not provided, all volumes are included.
        volumes:
          - volume_name: "data_volume"
          - volume_name: "temp_volume"
      - schema_name: "bonze_3"
        # Optional: exclude specific tables in the schema. if not provided, all tables are included.
        exclude_tables:
          - table_name: "temp_table"
        # Optional: exclude specific volumes in the schema. if not provided, all volumes are included.
        exclude_volumes:
          - volume_name: "old_volume"
      - schema_name: "bonze_4"
      # Optional: WHERE clause condition to select specific tables from {catalog}.{schema}
        # config table_filter_expression and tables at the same time not allowed.
        table_filter_expression: "table_name like 'fact_%' or table_name like 'dim_%'"
    # Optional: exclude_schema list.
    exclude_schemas:
      # Optional: exclude specific schemas in the catalog.
      - schema_name: "bonze_exclude_1"
      - schema_name: "bonze_exclude_2"
    # Optional: backup configuration (at catalog level, overrides replication group level config)
    backup_config:
    # check config at replication group level
    # Optional: replication configuration (at catalog level, overrides replication group level config)
    replication_config:
    # check config at replication group level
    # Optional: reconciliation configuration (at catalog level, overrides replication group level config)
    reconciliation_config:
    # check config at replication group level

# Optional: Concurrency settings at replication group level
concurrency:
  # Optional: whether to process schemas in serial. default is false. all tables in the catalog will be processed in parallel if set to false.
  process_schemas_in_serial: false
  # Optional: no of parallel threads for table and volume replication. default is 8.
  max_workers: 8
  # Optional: no of parallel threads for table filtering in a schema. default is 8.
  parallel_table_filter: 8
  # Optional: timeout for each task. default is 3600.
  timeout_seconds: 1800

# Optional: Retry settings at replication group level
retry:
  # Optional: maximum number of retry attempts on object cloning operations. default is 3.
  max_attempts: 3
  # Optional: delay between retry attempts in seconds. default is 5.
  retry_delay_seconds: 5

# Optional: Logging settings
logging:
  # Optional: logging level. default is "INFO".
  # Supported levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: "INFO"
  # Optional: log format. default is "json".
  # Supported formats: "text" or "json"
  format: "json"
  # Optional: whether to log to file. default is false.
  log_to_file: false
  # Optional: log file path. default is None.
  # Required if log_to_file is true
  log_file_path: null
